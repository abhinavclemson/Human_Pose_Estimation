{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the model to be used\n",
    "COCO and MPI are body pose estimation model. COCO has 18 points and MPI has 15 points as output.\n",
    "\n",
    "HAND is hand keypoints estimation model. It has 22 points as output\n",
    "\n",
    "Ensure that the model files are available in the folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = \"MPI\"\n",
    "\n",
    "if MODE is \"COCO\":\n",
    "    protoFile = \"/Users/abhi/Desktop/OpenPose/pose/coco/pose_deploy_linevec.prototxt.txt\"\n",
    "    weightsFile = \"/Users/abhi/Desktop/OpenPose/pose/coco/pose_iter_440000.caffemodel\"\n",
    "    nPoints = 18\n",
    "    POSE_PAIRS = [ [1,0],[1,2],[1,5],[2,3],[3,4],[5,6],[6,7],[1,8],[8,9],[9,10],[1,11],[11,12],[12,13],[0,14],[0,15],[14,16],[15,17]]\n",
    "\n",
    "elif MODE is \"MPI\" :\n",
    "    protoFile = \"Users/abhi/Desktop/OpenPose/pose/mpi/pose_deploy_linevec_faster_4_stages.prototxt\"\n",
    "    weightsFile = \"Users/abhi/Desktop/OpenPose/pose/mpi/pose_iter_160000.caffemodel\"\n",
    "    nPoints = 15\n",
    "    POSE_PAIRS = [[0,1], [1,2], [2,3], [3,4], [1,5], [5,6], [6,7], [1,14], [14,8], [8,9], [9,10], [14,11], [11,12], [12,13] ]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us load an image with multiple people and check what the model sees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users/abhi/Desktop/OpenPose/pose/mpi/pose_deploy_linevec_faster_4_stages.prototxt\n",
      "Users/abhi/Desktop/OpenPose/pose/mpi/pose_iter_160000.caffemodel\n"
     ]
    }
   ],
   "source": [
    "image1 = cv2.imread(\"multiple.jpeg\")\n",
    "frameWidth = image1.shape[1]\n",
    "frameHeight = image1.shape[0]\n",
    "threshold = 0.1\n",
    "\n",
    "print(protoFile)\n",
    "print(weightsFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the network and pass the image through the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.2.0) /Users/travis/build/skvark/opencv-python/opencv/modules/dnn/src/caffe/caffe_io.cpp:1121: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"Users/abhi/Desktop/OpenPose/pose/mpi/pose_deploy_linevec_faster_4_stages.prototxt\" in function 'ReadProtoFromTextFile'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-dceca213638f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadNetFromCaffe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotoFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweightsFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minWidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m368\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minHeight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m368\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m inpBlob = cv2.dnn.blobFromImage(image1, 1.0 / 255, (inWidth, inHeight),\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.2.0) /Users/travis/build/skvark/opencv-python/opencv/modules/dnn/src/caffe/caffe_io.cpp:1121: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"Users/abhi/Desktop/OpenPose/pose/mpi/pose_deploy_linevec_faster_4_stages.prototxt\" in function 'ReadProtoFromTextFile'\n"
     ]
    }
   ],
   "source": [
    "net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)\n",
    "\n",
    "inWidth = 368\n",
    "inHeight = 368\n",
    "inpBlob = cv2.dnn.blobFromImage(image1, 1.0 / 255, (inWidth, inHeight),\n",
    "                          (0, 0, 0), swapRB=False, crop=False)\n",
    "\n",
    "net.setInput(inpBlob)\n",
    "output = net.forward()\n",
    "H = output.shape[2]\n",
    "W = output.shape[3]\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slice a probability map from the output for a specific keypoint and plot the heatmap ( after resizing ) on the image itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 5\n",
    "probMap = output[0, i, :, :]\n",
    "probMap = cv2.resize(probMap, (image1.shape[1], image1.shape[0]))\n",
    "plt.figure(figsize=[14,10])\n",
    "plt.imshow(cv2.cvtColor(image1, cv2.COLOR_BGR2RGB))\n",
    "plt.imshow(probMap, alpha=0.6)\n",
    "plt.colorbar()\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarly plot the affinity map on the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 24\n",
    "probMap = output[0, i, :, :]\n",
    "probMap = cv2.resize(probMap, (image1.shape[1], image1.shape[0]))\n",
    "plt.figure(figsize=[14,10])\n",
    "plt.imshow(cv2.cvtColor(image1, cv2.COLOR_BGR2RGB))\n",
    "plt.imshow(probMap, alpha=0.6)\n",
    "plt.colorbar()\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we find the keypoints for a image with only single person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = cv2.imread(\"single.jpeg\")\n",
    "frameCopy = np.copy(frame)\n",
    "frameWidth = frame.shape[1]\n",
    "frameHeight = frame.shape[0]\n",
    "threshold = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pass it through the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpBlob = cv2.dnn.blobFromImage(frame, 1.0 / 255, (inWidth, inHeight),\n",
    "                          (0, 0, 0), swapRB=False, crop=False)\n",
    "\n",
    "net.setInput(inpBlob)\n",
    "\n",
    "output = net.forward()\n",
    "H = output.shape[2]\n",
    "W = output.shape[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gather the points and plot the keypoints and the skeleton figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Empty list to store the detected keypoints\n",
    "points = []\n",
    "\n",
    "for i in range(nPoints):\n",
    "    # confidence map of corresponding body's part.\n",
    "    probMap = output[0, i, :, :]\n",
    "\n",
    "    # Find global maxima of the probMap.\n",
    "    minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\n",
    "    \n",
    "    # Scale the point to fit on the original image\n",
    "    x = (frameWidth * point[0]) / W\n",
    "    y = (frameHeight * point[1]) / H\n",
    "\n",
    "    if prob > threshold : \n",
    "        cv2.circle(frameCopy, (int(x), int(y)), 8, (0, 255, 255), thickness=-1, lineType=cv2.FILLED)\n",
    "        cv2.putText(frameCopy, \"{}\".format(i), (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, lineType=cv2.LINE_AA)\n",
    "        cv2.circle(frame, (int(x), int(y)), 8, (0, 0, 255), thickness=-1, lineType=cv2.FILLED)\n",
    "\n",
    "        # Add the point to the list if the probability is greater than the threshold\n",
    "        points.append((int(x), int(y)))\n",
    "    else :\n",
    "        points.append(None)\n",
    "\n",
    "# Draw Skeleton\n",
    "for pair in POSE_PAIRS:\n",
    "    partA = pair[0]\n",
    "    partB = pair[1]\n",
    "\n",
    "    if points[partA] and points[partB]:\n",
    "        cv2.line(frame, points[partA], points[partB], (0, 255, 255), 3)\n",
    "\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.imshow(cv2.cvtColor(frameCopy, cv2.COLOR_BGR2RGB))\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
